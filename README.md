# Trimble Geospatial Demo

This repo contains a .NET 8 API and Databricks pipelines for processing LiDAR point
cloud data into tiles, surface models, and derived features (water bodies and
building candidates).

## Data source
Lidar was captured for Wellington City Council by Aerial Surveys between 2019 and
2020. The dataset was generated by Aerial Surveys and their subcontractors. The
survey area includes Wellington City and the surrounding area. Data management and
distribution is by Land Information New Zealand.

Dataset summary:
- Platform: Airborne LiDAR
- Survey area: 158 km2
- Point density: 23.29 pts/m2
- Survey date: 03/20/2019 - 03/14/2020

Metadata and licensing:
- Use license: CC BY 4.0
- Funder: WCC
- Partner: LINZ
- Collector: AS

Source: https://portal.opentopography.org/lidarDataset?opentopoID=OTLAS.092020.2193.1

## Upload api architecture

This diagram shows the end-to-end ingestion path for a single LAZ upload:

- The client requests an upload session from the API and uploads directly to ADLS via SAS.
- When the client marks the upload complete, the API validates the blob, records status in the Job DB, and enqueues a `job_init` message.
- A Function/worker consumes the queue message and triggers the Databricks job, which reads raw data from staging and writes Delta tables to Unity Catalog.
- The client polls the API for job status.

```mermaid
sequenceDiagram
  autonumber
  participant Client as Client
  participant AAD as Azure AD
  participant API as Upload&Job API (.NET 6)
  participant DB as Job Metadata DB
  participant STG as Blob/ADLS Staging
  participant SB as Service Bus
  participant ORCH as Orchestrator (Function/Worker)
  participant DBX as Databricks Jobs API
  participant DL as Delta/Unity Catalog

  Client->>AAD: Login / get access token (JWT)
  Client->>API: POST /v1/uploads (JWT)
  API->>DB: Create job + upload session (CREATED)
  API->>STG: Generate SAS (scoped path, TTL)
  API-->>Client: {uploadUrl(SAS), uploadId, jobId, expiresAt}

  Client->>STG: Upload LAZ via SAS (direct PUT/blocks)
  Client->>API: POST /v1/uploads/{uploadId}/complete
  API->>STG: Validate blob exists (etag/size optional)
  API->>DB: Update status -> UPLOADED -> QUEUED
  API->>SB: Send message {jobId, stagingUri, ...}

  ORCH->>SB: Receive job_init message
  ORCH->>DB: Update status -> DBX_TRIGGERED
  ORCH->>DBX: run-now(jobId, stagingUri, siteId, scanId)
  DBX-->>ORCH: runId
  ORCH->>DB: Persist runId

  DBX->>STG: Read LAZ raw
  DBX->>DL: Process + write Delta tables
  DBX-->>ORCH: Completion (or ORCH polls run status)
  ORCH->>DB: Update status -> SUCCEEDED/FAILED

  Client->>API: GET /v1/jobs/{jobId}
  API->>DB: Read job status + pointers
  API-->>Client: status, runId, error, result pointers
```

```mermaid
flowchart LR
  %% Clients & Identity
  C[Client App] -->|OAuth2 / OIDC| AAD[Azure AD / Entra ID]
  C -->|JWT Bearer| API[Upload & Job API]

  %% Data & Metadata
  API -->|Create Upload Session| JDB[(Job Metadata DB)]
  API -->|Generate SAS write-only| STG[(Blob/ADLS Gen2)]
  C -->|Direct upload via SAS| STG

  %% Complete + Queue
  C -->|Complete upload| API
  API -->|Verify blob exists| STG
  API -->|Enqueue job_init msg| SB[(Azure Service Bus Queue/Topic)]

  %% Orchestration -> Databricks
  SB -->|Consume job_init| ORCH[Job Orchestrator]
  ORCH -->|Update status: DBX_TRIGGERED| JDB
  ORCH -->|Run-now| DBX[Azure Databricks Jobs API]

  %% Processing & Output
  DBX -->|Read raw LAZ from staging| STG
  DBX -->|Write processed tables| DL[(Delta Lake / Unity Catalog)]
  DBX -->|Emit completion event or Orchestrator polls run state| ORCH
  ORCH -->|Update status: SUCCEEDED/FAILED| JDB

  %% Query results (optional extension)
  QAPI[Query API] --> DL
  C --> QAPI
```

### Permissions (high level)
This is a simplified view of the identities involved, plus the auth mechanism used in this repo:

- Client -> API: API key via `x-api-key` header (see `ApiKeyMiddleware`).
- API -> Job DB (Azure SQL): `Authentication=Active Directory Default` (uses App Service Managed Identity in Azure; local dev uses your signed-in developer identity).
- API -> ADLS/Blob: issues short-lived write SAS.
  - Preferred: User Delegation SAS via `DefaultAzureCredential` (Managed Identity in Azure).
  - Fallback: storage account key (`STORAGE_ACCOUNT_KEY`) if User Delegation SAS is unavailable.
- API -> Service Bus: Shared Access Signature (SAS) via connection string (`SERVICEBUS__CONNECTION`).

Note: the sequence diagram shows JWT/OIDC as a possible client auth flow, but the current API implementation uses `x-api-key`.

```mermaid
flowchart TB
  %% Principals
  Client["Client (User)"]
  ApiMI["API App Service Identity"]
  FuncMI["Orchestrator Function Identity"]
  DbxId["Databricks Identity (SPN / Workspace Identity)"]

  %% Resources
  AAD["Entra ID"]
  Storage["ADLS Gen2 / Blob Storage"]
  Sql["Job Metadata DB (Azure SQL)"]
  ServiceBus["Service Bus (job-init)"]
  Databricks["Azure Databricks Jobs API"]
  Unity["Unity Catalog / Delta Tables"]

  %% Auth / permissions
  Client -- "(optional) OIDC login" --> AAD
  Client -- "API key (x-api-key)" --> ApiMI

  ApiMI -- "Generate SAS (scoped TTL)" --> Storage
  Client -- "Upload via SAS" --> Storage

  ApiMI -- "Read/write job status" --> Sql
  ApiMI -- "Send job-init" --> ServiceBus

  FuncMI -- "Listen job-init" --> ServiceBus
  FuncMI -- "Update runId/status" --> Sql
  FuncMI -- "Run Now" --> Databricks

  DbxId -- "Read raw" --> Storage
  DbxId -- "Write tables" --> Unity
```

## Databricks job flow (overview)
All processing runs in Databricks using notebooks in `databricks/pipelines`. The
workflow is designed as a DAG in Databricks Workflows and runs in this order:

```mermaid
graph LR
  A[01_ingest_raw] --> B[02_spatial_tiling_v2]
  B --> C[03_tile_stats_v2]
  C --> D[04_surface_cells_v2]
  D --> E[04_surface_patches_v2]
  D --> F[05_feature_water_bodies_v2]
  E --> G[05_feature_building_candidates_v2]
  F --> H[99_optimize_tables]
  G --> H
```

1) 01_ingest_raw (no dependency)
2) 02_spatial_tiling_v2 depends on 01_ingest_raw
3) 03_tile_stats_v2 depends on 02_spatial_tiling_v2
4) 04_surface_cells_v2 depends on 03_tile_stats_v2
5) 04_surface_patches_v2 depends on 04_surface_cells_v2
6) 05_feature_water_bodies_v2 depends on 04_surface_cells_v2
7) 05_feature_building_candidates_v2 depends on 04_surface_patches_v2
8) 99_optimize_tables depends on both feature tasks

Goal:
Raw -> tiling -> tile stats -> surface cells -> surface patches -> features -> optimize

### Pipeline stages and tables
All tables are in Unity Catalog `main.demo`.

- Ingest:
  - Input: raw point cloud files
  - Output: `points_raw`
- Tiling:
  - Input: `points_raw`
  - Output: `processed_points_tiled_v2`
- Aggregation:
  - Input: `processed_points_tiled_v2`
  - Output: `tile_stats_v2`
- Surface:
  - Input: `processed_points_tiled_v2`, `tile_stats_v2`
  - Output: `surface_cells_v2`, `surface_patches_v2`
- Features:
  - Input: `surface_cells_v2`, `surface_patches_v2`
  - Output: `features_water_bodies_v2`, `features_building_candidates_v2`
- Optimization:
  - Input: all above tables
  - Output: optimized Delta files and statistics

### Notebooks by stage
See `databricks/pipelines` for the implementation. Key notebooks:

- `00_setup/`
  - `setup_external_locations.ipynb`
  - `setup_tiling_params.ipynb`
  - `create_site_lock_table.ipynb`
- `01_ingest/01_ingest_raw.ipynb`
- `02_processing/02_spatial_tiling_v2.ipynb`
- `03_aggregation/03_tile_stats_v2.ipynb`
- `04_surface/04_surface_cells_v2.ipynb`
- `04_surface/04_surface_patches_v2.ipynb`
- `05_feature/05_feature_water_bodies_v2.ipynb`
- `05_feature/05_feature_building_candidates_v2.ipynb`
- `99_optimization/99_optimize_tables.ipynb`

### Job parameters (defaults)
Configured at the job level and passed to notebooks via widgets or spark config:

- `siteId`: `wellington_cbd`
- `cellSizeM`: `0.5`
- `patchWaterThreshold`: `0.7`
- `skipWaterTileRatio`: `0.8`
- `targetIngestRunId`: empty (optional)

## API
The API is a .NET 8 ASP.NET Core Web API that serves the processed data.
Base path: `/api/v1`.

Swagger UI:
- Local: `http://localhost:<port>/swagger`
- Deployed: `https://<app>.azurewebsites.net/swagger`

All endpoints require an API key in the `x-api-key` header.

## Repo structure (high level)
- `api/`: ASP.NET Core API, Swagger, and data access
- `databricks/`: Databricks notebooks and workflow definitions
- `data-model/`, `architecture/`, `decisions/`: design docs and references

## Images
<img width="2545" height="1293" alt="image" src="https://github.com/user-attachments/assets/c71aee4a-eca5-4972-bfa3-0275df1c9ee5" />
<img width="2340" height="1245" alt="image" src="https://github.com/user-attachments/assets/4d0b0d27-1d66-4e27-abcc-e992646df97a" />
<img width="2338" height="898" alt="image" src="https://github.com/user-attachments/assets/b7c5dba1-8877-4e21-be47-53ee92c8c3ce" />
