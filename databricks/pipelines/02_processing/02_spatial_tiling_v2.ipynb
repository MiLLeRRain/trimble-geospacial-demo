{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e8c548-d346-4059-86d4-7c7430d4add5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_spatial_tiling_v2.py\n",
    "import json\n",
    "from pyspark.sql import functions as F, Window\n",
    "from datetime import datetime\n",
    "from trimble_geospatial_demo_utils.site_lock import acquire_site_lock, release_site_lock\n",
    "from trimble_geospatial_demo_utils import send_notification\n",
    "\n",
    "# ==================================================\n",
    "# Unity Catalog context\n",
    "# ==================================================\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA demo\")\n",
    "\n",
    "# ==================================================\n",
    "# Read Job Parameters\n",
    "# ==================================================\n",
    "dbutils.widgets.text(\"siteId\", \"\", \"Site ID\")\n",
    "dbutils.widgets.text(\"ingestRunId\", \"\", \"Ingest Run ID\")\n",
    "dbutils.widgets.text(\"uploadJobId\", \"\", \"Upload Job ID\")\n",
    "dbutils.widgets.text(\"notificationUrl\", \"\", \"Notification URL\")\n",
    "dbutils.widgets.text(\"dbxWebhookSecret\", \"\", \"DBX Webhook Secret\")\n",
    "\n",
    "SITE_ID = dbutils.widgets.get(\"siteId\")\n",
    "TARGET_INGEST_RUN_ID = dbutils.widgets.get(\"ingestRunId\")\n",
    "UPLOAD_JOB_ID = dbutils.widgets.get(\"uploadJobId\")\n",
    "NOTIFICATION_URL = dbutils.widgets.get(\"notificationUrl\")\n",
    "DBX_WEBHOOK_SECRET = dbutils.widgets.get(\"dbxWebhookSecret\")\n",
    "\n",
    "# Validate required parameters\n",
    "if not SITE_ID:\n",
    "    raise ValueError(\"Missing required job parameter: siteId\")\n",
    "if not TARGET_INGEST_RUN_ID:\n",
    "    raise ValueError(\"Missing required job parameter: ingestRunId\")\n",
    "\n",
    "print(f\"üèóÔ∏è  Site ID: {SITE_ID}\")\n",
    "print(f\"üîÑ Ingest Run ID: {TARGET_INGEST_RUN_ID}\")\n",
    "\n",
    "# ==================================================\n",
    "# CONFIG SWITCHES\n",
    "# ==================================================\n",
    "USE_CONTROL_TABLE = False\n",
    "ALLOW_FALLBACK = True\n",
    "\n",
    "# ==================================================\n",
    "# TABLES & PATHS (UC External Table)\n",
    "# ==================================================\n",
    "RAW_TABLE = \"points_raw\"  # main.demo.points_raw\n",
    "\n",
    "PROCESSED_TABLE_V2 = \"processed_points_tiled_v2\"\n",
    "PROCESSED_PATH_V2  = \"abfss://processed@trimblegeospatialdemo.dfs.core.windows.net/points_tiled_v2\"\n",
    "\n",
    "CONTROL_TABLE = \"control_tiling_params\"  # main.demo.control_tiling_params\n",
    "\n",
    "# ==================================================\n",
    "# CODE DEFAULTS (SAFE BASELINE)\n",
    "# ==================================================\n",
    "DEFAULT_TILE_SIZE_M = 25.0\n",
    "DEFAULT_HOT_TILE_THRESHOLD = 100_000\n",
    "DEFAULT_TARGET_POINTS_PER_BUCKET = 600_000\n",
    "DEFAULT_SALT_BUCKETS = 16\n",
    "DEFAULT_ENABLE_SALT = False\n",
    "\n",
    "# ==================================================\n",
    "# Get Job Run ID (for lock tracking)\n",
    "# ==================================================\n",
    "JOB_RUN_ID = spark.conf.get(\n",
    "    \"spark.databricks.job.runId\",\n",
    "    \"manual-notebook\"\n",
    ")\n",
    "\n",
    "print(f\"Job Run ID: {JOB_RUN_ID}\")\n",
    "\n",
    "# ==================================================\n",
    "# Acquire site lock before processing\n",
    "# ==================================================\n",
    "print(f\"\\n=== Acquiring lock for site: {SITE_ID} ===\")\n",
    "acquire_site_lock(\n",
    "    spark=spark,\n",
    "    site_id=SITE_ID,\n",
    "    locked_by=JOB_RUN_ID,\n",
    "    ttl_minutes=90\n",
    ")\n",
    "print(f\"‚úÖ Lock acquired for site: {SITE_ID}\")\n",
    "\n",
    "try:\n",
    "    # ==================================================\n",
    "    # 1) Load raw from Unity Catalog & use specified ingestRunId\n",
    "    # ==================================================\n",
    "    df_raw_site = (\n",
    "        spark.table(RAW_TABLE)\n",
    "             .filter(F.col(\"siteId\") == SITE_ID)\n",
    "    )\n",
    "\n",
    "    INGEST_RUN_ID = TARGET_INGEST_RUN_ID\n",
    "\n",
    "    df_raw = df_raw_site.filter(F.col(\"ingestRunId\") == INGEST_RUN_ID)\n",
    "\n",
    "    print(\"Using siteId =\", SITE_ID)\n",
    "    print(\"Using ingestRunId =\", INGEST_RUN_ID)\n",
    "\n",
    "    # ==================================================\n",
    "    # 2) Resolve parameters (override ‚Üí control ‚Üí default)\n",
    "    # ==================================================\n",
    "    param_source = \"code-defaults\"\n",
    "\n",
    "    TILE_SIZE_M = DEFAULT_TILE_SIZE_M\n",
    "    HOT_TILE_THRESHOLD = DEFAULT_HOT_TILE_THRESHOLD\n",
    "    TARGET_POINTS_PER_BUCKET = DEFAULT_TARGET_POINTS_PER_BUCKET\n",
    "    SALT_BUCKETS = DEFAULT_SALT_BUCKETS\n",
    "    ENABLE_SALT = DEFAULT_ENABLE_SALT\n",
    "\n",
    "    if USE_CONTROL_TABLE:\n",
    "        try:\n",
    "            params = (\n",
    "                spark.table(CONTROL_TABLE)\n",
    "                     .filter((F.col(\"siteId\") == SITE_ID) & (F.col(\"ingestRunId\") == INGEST_RUN_ID))\n",
    "                     .orderBy(F.col(\"computedAt\").desc())\n",
    "                     .limit(1)\n",
    "                     .collect()\n",
    "            )\n",
    "\n",
    "            if params:\n",
    "                p = params[0]\n",
    "                TILE_SIZE_M = float(p[\"tileSizeM\"])\n",
    "                HOT_TILE_THRESHOLD = int(p[\"hotTileThreshold\"])\n",
    "                TARGET_POINTS_PER_BUCKET = int(p[\"targetPointsPerBucket\"])\n",
    "                SALT_BUCKETS = int(p[\"saltBuckets\"])\n",
    "                ENABLE_SALT = int(p[\"maxTilePoints\"]) >= TARGET_POINTS_PER_BUCKET\n",
    "                param_source = \"control-table\"\n",
    "            else:\n",
    "                if not ALLOW_FALLBACK:\n",
    "                    raise ValueError(\"No control table entry found and fallback disabled.\")\n",
    "                print(\"‚ö†Ô∏è No control params found; falling back to code defaults.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if not ALLOW_FALLBACK:\n",
    "                raise\n",
    "            print(\"‚ö†Ô∏è Failed to read control table, falling back to code defaults.\")\n",
    "            print(\"Reason:\", str(e)[:200])\n",
    "\n",
    "    print(\"=== Spatial Tiling Parameters (V2) ===\")\n",
    "    print(\"Source:\", param_source)\n",
    "    print(\"TILE_SIZE_M =\", TILE_SIZE_M)\n",
    "    print(\"HOT_TILE_THRESHOLD =\", HOT_TILE_THRESHOLD)\n",
    "    print(\"TARGET_POINTS_PER_BUCKET =\", TARGET_POINTS_PER_BUCKET)\n",
    "    print(\"SALT_BUCKETS =\", SALT_BUCKETS)\n",
    "    print(\"ENABLE_SALT =\", ENABLE_SALT)\n",
    "    print(\"======================================\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 3) Compute origin (bbox min) & tile indices\n",
    "    #    NOTE: origin derived from THIS RUN to ensure determinism within run.\n",
    "    #    If you need cross-run stable tiling, store origin per site in a control table.\n",
    "    # ==================================================\n",
    "    origin_row = df_raw.agg(F.min(\"x\").alias(\"minX\"), F.min(\"y\").alias(\"minY\")).first()\n",
    "    originX, originY = float(origin_row[\"minX\"]), float(origin_row[\"minY\"])\n",
    "\n",
    "    df_tiled = (\n",
    "        df_raw\n",
    "        .withColumn(\"originX\", F.lit(originX))\n",
    "        .withColumn(\"originY\", F.lit(originY))\n",
    "        .withColumn(\"tileSizeM\", F.lit(float(TILE_SIZE_M)))\n",
    "        .withColumn(\"tileX\", F.floor((F.col(\"x\") - F.lit(originX)) / F.lit(TILE_SIZE_M)).cast(\"int\"))\n",
    "        .withColumn(\"tileY\", F.floor((F.col(\"y\") - F.lit(originY)) / F.lit(TILE_SIZE_M)).cast(\"int\"))\n",
    "        .withColumn(\"tileId\", F.concat_ws(\"_\", F.col(\"tileX\").cast(\"string\"), F.col(\"tileY\").cast(\"string\")))\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 4) Salt only if enabled\n",
    "    # ==================================================\n",
    "    if ENABLE_SALT:\n",
    "        tile_counts = (\n",
    "            df_tiled.groupBy(\"tileId\", \"tileX\", \"tileY\")\n",
    "                    .agg(F.count(\"*\").alias(\"pointCount\"))\n",
    "        )\n",
    "\n",
    "        hot_keys = (\n",
    "            tile_counts.filter(F.col(\"pointCount\") >= HOT_TILE_THRESHOLD)\n",
    "                       .select(\"tileId\")\n",
    "                       .distinct()\n",
    "        )\n",
    "\n",
    "        hot_keys_b = F.broadcast(hot_keys)\n",
    "\n",
    "        df_hot = (\n",
    "            df_tiled.join(hot_keys_b, [\"tileId\"], \"left_semi\")\n",
    "                    .withColumn(\"tileSalt\", F.pmod(F.hash(\"x\",\"y\",\"z\"), F.lit(SALT_BUCKETS)))\n",
    "                    .withColumn(\"isHotTile\", F.lit(1))\n",
    "        )\n",
    "\n",
    "        df_non_hot = (\n",
    "            df_tiled.join(hot_keys_b, [\"tileId\"], \"left_anti\")\n",
    "                    .withColumn(\"tileSalt\", F.lit(0))\n",
    "                    .withColumn(\"isHotTile\", F.lit(0))\n",
    "        )\n",
    "\n",
    "        df_processed = df_hot.unionByName(df_non_hot)\n",
    "    else:\n",
    "        df_processed = (\n",
    "            df_tiled\n",
    "            .withColumn(\"tileSalt\", F.lit(0))\n",
    "            .withColumn(\"isHotTile\", F.lit(0))\n",
    "        )\n",
    "\n",
    "    # ==================================================\n",
    "    # 5) Add snapshot metadata (traceability)\n",
    "    #    Keep ingestRunId as snapshot marker + add snapshotAt timestamp\n",
    "    # ==================================================\n",
    "    df_processed = (\n",
    "        df_processed\n",
    "        .withColumn(\"snapshotAt\", F.current_timestamp())  # When this snapshot was created\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== Snapshot Metadata ===\")\n",
    "    print(f\"  - ingestRunId: {INGEST_RUN_ID} (source data run)\")\n",
    "    print(f\"  - snapshotAt: {datetime.now().isoformat()} (processing timestamp)\")\n",
    "    print(f\"  - Strategy: Latest snapshot only (previous snapshots overwritten)\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 6) Write V2: Only keep latest data per site\n",
    "    #    Strategy: replaceWhere by siteId only (overwrite entire site)\n",
    "    #    Keep ingestRunId + snapshotAt for traceability\n",
    "    # ==================================================\n",
    "    print(f\"\\n=== Writing V2 to Unity Catalog table: {PROCESSED_TABLE_V2} ===\")\n",
    "\n",
    "    (\n",
    "        df_processed.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"replaceWhere\", f\"siteId = '{SITE_ID}'\")  # Only filter by siteId (replace entire site)\n",
    "            .option(\"path\", PROCESSED_PATH_V2)\n",
    "            .partitionBy(\"siteId\", \"tileId\")\n",
    "            .saveAsTable(PROCESSED_TABLE_V2)\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ V2 Data written and table registered in Unity Catalog\")\n",
    "    print(f\"   - Table: main.demo.{PROCESSED_TABLE_V2}\")\n",
    "    print(f\"   - Location: {PROCESSED_PATH_V2}\")\n",
    "    print(f\"   - Strategy: Only latest ingestRunId ({INGEST_RUN_ID}) kept for site '{SITE_ID}'\")\n",
    "    print(f\"   - Snapshot timestamp: {datetime.now().isoformat()}\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 7) Verify V2 write (snapshot metadata)\n",
    "    # ==================================================\n",
    "    print(\"\\n=== Verify V2 table (snapshot metadata) ===\")\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "      siteId,\n",
    "      ingestRunId,\n",
    "      MIN(snapshotAt) AS snapshotAt,\n",
    "      COUNT(*) AS rows,\n",
    "      COUNT(DISTINCT tileId) AS tiles\n",
    "    FROM {PROCESSED_TABLE_V2}\n",
    "    WHERE siteId = '{SITE_ID}'\n",
    "    GROUP BY siteId, ingestRunId\n",
    "    ORDER BY ingestRunId DESC\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    print(\"\\n=== Sample data with metadata ===\")\n",
    "    spark.sql(f\"\"\"\n",
    "    SELECT siteId, ingestRunId, tileId, tileX, tileY, snapshotAt\n",
    "    FROM {PROCESSED_TABLE_V2}\n",
    "    WHERE siteId = '{SITE_ID}'\n",
    "    LIMIT 5\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    print(\"\\n‚úÖ Complete (V2)!\")\n",
    "    print(\"\\nSnapshot design benefits:\")\n",
    "    print(\"  ‚úÖ ingestRunId: Tracks source data run\")\n",
    "    print(\"  ‚úÖ snapshotAt: Tracks when this snapshot was created\")\n",
    "    print(\"  ‚úÖ Latest snapshot only: No historical data accumulation\")\n",
    "    print(\"  ‚úÖ API-friendly: Easy to return metadata with results\")\n",
    "    print(\"  ‚úÖ Troubleshooting: Can trace back to source data\")\n",
    "\n",
    "except Exception as e:\n",
    "    if NOTIFICATION_URL and DBX_WEBHOOK_SECRET:\n",
    "        payload = {\n",
    "            \"runId\": spark.conf.get(\"spark.databricks.job.runId\", \"manual-notebook\"),\n",
    "            \"jobId\": UPLOAD_JOB_ID,\n",
    "            \"status\": \"FAILED\",\n",
    "            \"error\": str(e),\n",
    "            \"siteId\": SITE_ID,\n",
    "            \"ingestRunId\": TARGET_INGEST_RUN_ID,\n",
    "        }\n",
    "        try:\n",
    "            send_notification(json.dumps(payload), NOTIFICATION_URL, webhook_secret=DBX_WEBHOOK_SECRET)\n",
    "        except Exception as notify_ex:\n",
    "            print(\"‚ö†Ô∏è Notification failed:\", str(notify_ex)[:200])\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # ==================================================\n",
    "    # Release site lock (always executed, even on error)\n",
    "    # ==================================================\n",
    "    print(f\"\\n=== Releasing lock for site: {SITE_ID} ===\")\n",
    "    release_site_lock(\n",
    "        spark=spark,\n",
    "        site_id=SITE_ID,\n",
    "        locked_by=JOB_RUN_ID\n",
    "    )\n",
    "    print(f\"‚úÖ Lock released for site: {SITE_ID}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_spatial_tiling_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
