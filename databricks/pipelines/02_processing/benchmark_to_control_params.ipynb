{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c7c227c-4b73-4d8c-b9d7-6c57a50518a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Now import from utils package\n",
    "from utils.site_lock import acquire_site_lock, release_site_lock\n",
    "\n",
    "print(\"✅ Successfully imported site_lock functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb8110b-00fd-47b2-a8c1-3387fb51b0a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02A_benchmark_tiling_params.py NOT yet initialized\n",
    "from pyspark.sql import functions as F, Row\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set Unity Catalog context\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA demo\")\n",
    "\n",
    "# ===== Configuration =====\n",
    "RAW_PATH = \"abfss://raw@trimblegeospatialdemo.dfs.core.windows.net/points\"\n",
    "CONTROL_PATH = \"abfss://aggregated@trimblegeospatialdemo.dfs.core.windows.net/control/tiling_params\"\n",
    "\n",
    "# Unity Catalog table name\n",
    "CONTROL_TABLE = \"control_tiling_params\"  # main.demo.control_tiling_params\n",
    "\n",
    "SITE_ID = \"wellington_cbd\"\n",
    "TILE_SIZE_M = 25.0\n",
    "WATER_CLASS = 9\n",
    "\n",
    "# Benchmark knobs\n",
    "CANDIDATE_TARGETS = [50_000, 100_000, 150_000, 250_000, 400_000, 600_000]\n",
    "DESIRED_RUNTIME_SEC = 60\n",
    "ACCEPTABLE_RUNTIME_SEC = 120\n",
    "MAX_BUCKETS_CAP = 64\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 0) Check and setup External Location if needed\n",
    "# --------------------------------------------------\n",
    "print(\"=== Checking External Location setup ===\")\n",
    "\n",
    "# Check if aggregated_container external location exists\n",
    "try:\n",
    "    external_locations = spark.sql(\"SHOW EXTERNAL LOCATIONS\").collect()\n",
    "    aggregated_exists = any(loc.name == \"aggregated_container\" for loc in external_locations)\n",
    "    \n",
    "    if aggregated_exists:\n",
    "        print(\"✅ External Location 'aggregated_container' already exists\")\n",
    "    else:\n",
    "        print(\"⚠️ External Location 'aggregated_container' not found\")\n",
    "        print(\"Please create it manually in Databricks UI:\")\n",
    "        print(\"  1. Go to Catalog > External Data > External Locations\")\n",
    "        print(\"  2. Create Location:\")\n",
    "        print(\"     - Name: aggregated_container\")\n",
    "        print(\"     - URL: abfss://aggregated@trimblegeospatialdemo.dfs.core.windows.net/\")\n",
    "        print(\"     - Storage Credential: trimble_adls_credential\")\n",
    "        raise ValueError(\"External Location 'aggregated_container' must be created first\")\n",
    "except Exception as e:\n",
    "    if \"aggregated_container\" in str(e) and \"must be created\" in str(e):\n",
    "        raise\n",
    "    else:\n",
    "        print(f\"⚠️ Could not check external locations: {str(e)[:200]}\")\n",
    "        print(\"Proceeding anyway - if external location is missing, saveAsTable will fail with clear error\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1) Load raw + discover ingestRunId\n",
    "# --------------------------------------------------\n",
    "df_raw = (\n",
    "    spark.read.format(\"delta\").load(RAW_PATH)\n",
    "    .filter(F.col(\"siteId\") == SITE_ID)\n",
    "    .select(\"x\",\"y\",\"z\",\"classification\",\"siteId\",\"ingestRunId\")\n",
    ")\n",
    "\n",
    "ingest_run_ids = [r[\"ingestRunId\"] for r in df_raw.select(\"ingestRunId\").distinct().collect()]\n",
    "if len(ingest_run_ids) != 1:\n",
    "    raise ValueError(f\"Expected exactly one ingestRunId, found: {ingest_run_ids}\")\n",
    "\n",
    "INGEST_RUN_ID = ingest_run_ids[0]\n",
    "print(\"Using ingestRunId =\", INGEST_RUN_ID)\n",
    "\n",
    "df_raw = df_raw.filter(F.col(\"ingestRunId\") == INGEST_RUN_ID)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2) Compute tiling\n",
    "# --------------------------------------------------\n",
    "bbox = df_raw.agg(F.min(\"x\").alias(\"minX\"), F.min(\"y\").alias(\"minY\")).first()\n",
    "originX, originY = float(bbox[\"minX\"]), float(bbox[\"minY\"])\n",
    "\n",
    "df_tiled = (\n",
    "    df_raw\n",
    "    .withColumn(\"tileX\", F.floor((F.col(\"x\") - F.lit(originX)) / TILE_SIZE_M).cast(\"int\"))\n",
    "    .withColumn(\"tileY\", F.floor((F.col(\"y\") - F.lit(originY)) / TILE_SIZE_M).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) Tile distribution → HOT_TILE_THRESHOLD\n",
    "# --------------------------------------------------\n",
    "tile_counts = (\n",
    "    df_tiled.groupBy(\"tileX\",\"tileY\")\n",
    "    .agg(F.count(\"*\").alias(\"pointCount\"))\n",
    ").cache()\n",
    "\n",
    "tile_counts.count()\n",
    "\n",
    "qs = tile_counts.approxQuantile(\"pointCount\", [0.90, 0.95, 0.99, 0.995], 0.01)\n",
    "p90, p95, p99, p995 = qs\n",
    "max_points = tile_counts.agg(F.max(\"pointCount\")).first()[0]\n",
    "\n",
    "HOT_TILE_THRESHOLD = int(max(100_000, p99))\n",
    "hot_tiles = tile_counts.filter(F.col(\"pointCount\") >= HOT_TILE_THRESHOLD).count()\n",
    "\n",
    "print(f\"Tile distribution: p90={p90}, p95={p95}, p99={p99}, max={max_points}\")\n",
    "print(f\"Recommended HOT_TILE_THRESHOLD = {HOT_TILE_THRESHOLD}, hotTiles={hot_tiles}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) Benchmark hottest tile → targetPointsPerBucket\n",
    "# --------------------------------------------------\n",
    "hot_tile = tile_counts.orderBy(F.col(\"pointCount\").desc()).first()\n",
    "hot_x, hot_y, hot_count = hot_tile[\"tileX\"], hot_tile[\"tileY\"], hot_tile[\"pointCount\"]\n",
    "\n",
    "df_hot = (\n",
    "    df_tiled\n",
    "    .filter((F.col(\"tileX\")==hot_x) & (F.col(\"tileY\")==hot_y))\n",
    "    .cache()\n",
    ")\n",
    "df_hot.count()\n",
    "\n",
    "def run_benchmark(target):\n",
    "    buckets = max(1, min(MAX_BUCKETS_CAP, int((hot_count + target - 1) // target)))\n",
    "    t0 = time.time()\n",
    "\n",
    "    df_salted = df_hot.withColumn(\n",
    "        \"tileSalt\",\n",
    "        F.pmod(F.hash(\"x\",\"y\",\"z\"), F.lit(buckets))\n",
    "    )\n",
    "\n",
    "    df_salted.groupBy(\"tileSalt\").agg(\n",
    "        F.count(\"*\"),\n",
    "        F.min(\"z\"),\n",
    "        F.max(\"z\")\n",
    "    ).count()\n",
    "\n",
    "    return buckets, time.time() - t0\n",
    "\n",
    "results = []\n",
    "for t in CANDIDATE_TARGETS:\n",
    "    b, sec = run_benchmark(t)\n",
    "    results.append((t, b, sec))\n",
    "    print(f\"target={t:,} → buckets={b}, elapsed={sec:.2f}s\")\n",
    "\n",
    "feasible = [r for r in results if r[2] <= ACCEPTABLE_RUNTIME_SEC]\n",
    "best = max(feasible, key=lambda r: r[0]) if feasible else min(results, key=lambda r: r[2])\n",
    "\n",
    "TARGET_POINTS_PER_BUCKET, SALT_BUCKETS, _ = best\n",
    "\n",
    "print(\"Final choice:\",\n",
    "      \"targetPointsPerBucket =\", TARGET_POINTS_PER_BUCKET,\n",
    "      \"saltBuckets =\", SALT_BUCKETS)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5) Write control table to Unity Catalog using saveAsTable\n",
    "# --------------------------------------------------\n",
    "print(f\"\\n=== Writing control parameters to Unity Catalog ===\")\n",
    "\n",
    "# Create DataFrame with control parameters\n",
    "row = Row(\n",
    "    siteId=SITE_ID,\n",
    "    ingestRunId=INGEST_RUN_ID,\n",
    "    tileSizeM=float(TILE_SIZE_M),\n",
    "    hotTileThreshold=int(HOT_TILE_THRESHOLD),\n",
    "    targetPointsPerBucket=int(TARGET_POINTS_PER_BUCKET),\n",
    "    saltBuckets=int(SALT_BUCKETS),\n",
    "    maxTilePoints=int(max_points),\n",
    "    computedAt=datetime.now()\n",
    ")\n",
    "\n",
    "control_df = spark.createDataFrame([row])\n",
    "\n",
    "# Write to Unity Catalog using saveAsTable\n",
    "(\n",
    "    control_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"path\", CONTROL_PATH)  # Specify external location\n",
    "    .saveAsTable(CONTROL_TABLE)\n",
    ")\n",
    "\n",
    "print(f\"✅ Control parameters written and table registered in Unity Catalog\")\n",
    "print(f\"   - Table: main.demo.{CONTROL_TABLE}\")\n",
    "print(f\"   - Location: {CONTROL_PATH}\")\n",
    "\n",
    "# Verify table registration\n",
    "print(\"\\n=== Verify control table ===\")\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT siteId, ingestRunId, tileSizeM, hotTileThreshold, \n",
    "           targetPointsPerBucket, saltBuckets, maxTilePoints, computedAt\n",
    "    FROM {CONTROL_TABLE}\n",
    "    ORDER BY computedAt DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "print(\"\\n✅ Complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "benchmark_to_control_params",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
