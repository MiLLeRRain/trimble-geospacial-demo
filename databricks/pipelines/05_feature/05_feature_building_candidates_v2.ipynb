{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d7b7a3-0190-4003-ab56-6c3e63b005b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# 05_feature_building_candidates_v2.py\n",
    "from pyspark.sql import functions as F\n",
    "from utils.site_lock import acquire_site_lock, release_site_lock\n",
    "\n",
    "# ==================================================\n",
    "# Unity Catalog context\n",
    "# ==================================================\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA demo\")\n",
    "\n",
    "# ==================================================\n",
    "# CONFIG\n",
    "# ==================================================\n",
    "SITE_ID = spark.conf.get(\"pipeline.siteId\", \"wellington_cbd\")\n",
    "\n",
    "PATCHES_TABLE = \"surface_patches_v2\"\n",
    "OUTPUT_TABLE  = \"features_building_candidates_v2\"\n",
    "OUTPUT_PATH   = \"abfss://processed@trimblegeospatialdemo.dfs.core.windows.net/features_building_candidates_v2\"\n",
    "\n",
    "# Thresholds (tunable)\n",
    "MIN_PATCH_AREA_M2       = 20.0\n",
    "MIN_HEIGHT_ABOVE_GROUND = 2.5\n",
    "MAX_HEIGHT_RANGE        = 3.0\n",
    "MIN_POINTS_USED         = 200\n",
    "\n",
    "JOB_RUN_ID = spark.conf.get(\"spark.databricks.job.runId\", \"manual\")\n",
    "\n",
    "# ==================================================\n",
    "# Acquire site lock\n",
    "# ==================================================\n",
    "acquire_site_lock(\n",
    "    spark=spark,\n",
    "    site_id=SITE_ID,\n",
    "    locked_by=JOB_RUN_ID,\n",
    "    ttl_minutes=90\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ==================================================\n",
    "    # 1) Load surface patches (ground only)\n",
    "    # ==================================================\n",
    "    df_patches = (\n",
    "        spark.table(PATCHES_TABLE)\n",
    "             .filter(F.col(\"siteId\") == SITE_ID)\n",
    "             .filter(F.col(\"surfaceType\") == F.lit(\"ground\"))\n",
    "    )\n",
    "\n",
    "    if df_patches.rdd.isEmpty():\n",
    "        raise RuntimeError(\"No surface patches found\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 2) Estimate local ground reference per tile\n",
    "    #    (low percentile ground height)\n",
    "    # ==================================================\n",
    "    df_ground_ref = (\n",
    "        df_patches\n",
    "        .groupBy(\"siteId\", \"tileId\")\n",
    "        .agg(\n",
    "            F.expr(\"percentile_approx(minZ, 0.1)\").alias(\"groundZ_ref\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        df_patches\n",
    "        .join(df_ground_ref, [\"siteId\", \"tileId\"], \"left\")\n",
    "        .withColumn(\"heightAboveGround\", F.col(\"meanZ\") - F.col(\"groundZ_ref\"))\n",
    "        .withColumn(\"heightRange\", F.col(\"maxZ\") - F.col(\"minZ\"))\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 3) Candidate filtering (rule-based)\n",
    "    # ==================================================\n",
    "    df_candidates = (\n",
    "        df\n",
    "        .filter(F.col(\"pointsUsed\") >= F.lit(MIN_POINTS_USED))\n",
    "        .filter(F.col(\"heightAboveGround\") >= F.lit(MIN_HEIGHT_ABOVE_GROUND))\n",
    "        .filter(F.col(\"heightRange\") <= F.lit(MAX_HEIGHT_RANGE))\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 4) Compute final features\n",
    "    # ==================================================\n",
    "    df_features = (\n",
    "        df_candidates\n",
    "        .select(\n",
    "            \"siteId\",\n",
    "            \"tileId\",\n",
    "            F.col(\"patchId\").alias(\"buildingCandidateId\"),\n",
    "            \"pointsUsed\",\n",
    "            \"cellCount\",\n",
    "            \"minZ\",\n",
    "            \"meanZ\",\n",
    "            \"maxZ\",\n",
    "            \"heightAboveGround\",\n",
    "            \"heightRange\"\n",
    "        )\n",
    "        .withColumn(\"computedAt\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 5) Write latest snapshot by site\n",
    "    # ==================================================\n",
    "    (\n",
    "        df_features.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"replaceWhere\", f\"siteId = '{SITE_ID}'\")\n",
    "            .option(\"path\", OUTPUT_PATH)\n",
    "            .partitionBy(\"siteId\", \"tileId\")\n",
    "            .saveAsTable(OUTPUT_TABLE)\n",
    "    )\n",
    "\n",
    "    print(\"âœ… features_building_candidates_v2 written successfully\")\n",
    "\n",
    "finally:\n",
    "    release_site_lock(\n",
    "        spark=spark,\n",
    "        site_id=SITE_ID,\n",
    "        locked_by=JOB_RUN_ID\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_feature_building_candidates_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
