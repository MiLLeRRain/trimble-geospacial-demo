{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05567160-5b07-4a16-9abf-b07529ffc68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 05_feature_water_bodies_v2.py\n",
    "from pyspark.sql import functions as F\n",
    "from graphframes import GraphFrame\n",
    "from trimble_geospatial_demo_utils.site_lock import acquire_site_lock, release_site_lock\n",
    "\n",
    "# ==================================================\n",
    "# Unity Catalog context\n",
    "# ==================================================\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA demo\")\n",
    "\n",
    "# ==================================================\n",
    "# Read Job Parameters\n",
    "# ==================================================\n",
    "dbutils.widgets.text(\"siteId\", \"\", \"Site ID\")\n",
    "dbutils.widgets.text(\"tileSizeM\", \"\", \"Tile Size (meters)\")\n",
    "dbutils.widgets.text(\"cellSizeM\", \"\", \"Cell Size (meters)\")\n",
    "dbutils.widgets.text(\"waterCellThreshold\", \"\", \"Water Cell Threshold\")\n",
    "\n",
    "SITE_ID = dbutils.widgets.get(\"siteId\")\n",
    "TILE_SIZE_M = dbutils.widgets.get(\"tileSizeM\")\n",
    "CELL_SIZE_M = dbutils.widgets.get(\"cellSizeM\")\n",
    "WATER_CELL_THRESHOLD = dbutils.widgets.get(\"waterCellThreshold\")\n",
    "\n",
    "# Validate required parameters\n",
    "if not SITE_ID:\n",
    "    raise ValueError(\"Missing required job parameter: siteId\")\n",
    "if not TILE_SIZE_M:\n",
    "    raise ValueError(\"Missing required job parameter: tileSizeM\")\n",
    "if not CELL_SIZE_M:\n",
    "    raise ValueError(\"Missing required job parameter: cellSizeM\")\n",
    "if not WATER_CELL_THRESHOLD:\n",
    "    raise ValueError(\"Missing required job parameter: waterCellThreshold\")\n",
    "\n",
    "# Convert to appropriate types\n",
    "TILE_SIZE_M = float(TILE_SIZE_M)\n",
    "CELL_SIZE_M = float(CELL_SIZE_M)\n",
    "WATER_CELL_THRESHOLD = float(WATER_CELL_THRESHOLD)\n",
    "\n",
    "print(f\"üèóÔ∏è  Site ID: {SITE_ID}\")\n",
    "print(f\"üìè Tile Size: {TILE_SIZE_M}m\")\n",
    "print(f\"üìè Cell Size: {CELL_SIZE_M}m\")\n",
    "print(f\"üíß Water Cell Threshold: {WATER_CELL_THRESHOLD}\")\n",
    "\n",
    "# ==================================================\n",
    "# CONFIG\n",
    "# ==================================================\n",
    "CELLS_TABLE  = \"surface_cells_v2\"               # main.demo.surface_cells_v2\n",
    "OUTPUT_TABLE = \"features_water_bodies_v2\"       # main.demo.features_water_bodies_v2\n",
    "OUTPUT_PATH  = \"abfss://processed@trimblegeospatialdemo.dfs.core.windows.net/features_water_bodies_v2\"\n",
    "\n",
    "# Neighborhood type: 4-neighbour (recommended). If you want 8-neighbour later, add diagonals.\n",
    "USE_EIGHT_NEIGHBOUR = False\n",
    "\n",
    "JOB_RUN_ID = spark.conf.get(\"spark.databricks.job.runId\", \"manual-notebook\")\n",
    "\n",
    "# ==================================================\n",
    "# Set checkpoint directory for GraphFrames\n",
    "# Must use ABFSS path (Spark doesn't support /Volumes/ for checkpoints)\n",
    "# ==================================================\n",
    "CHECKPOINT_PATH = f\"abfss://processed@trimblegeospatialdemo.dfs.core.windows.net/checkpoints/graphframes/{SITE_ID}\"\n",
    "spark.sparkContext.setCheckpointDir(CHECKPOINT_PATH)\n",
    "\n",
    "# ==================================================\n",
    "# Derived constants\n",
    "# ==================================================\n",
    "cellsPerTile = int(round(TILE_SIZE_M / CELL_SIZE_M))\n",
    "if cellsPerTile <= 0:\n",
    "    raise ValueError(\"cellsPerTile computed <= 0. Check TILE_SIZE_M and CELL_SIZE_M.\")\n",
    "\n",
    "# ==================================================\n",
    "# Acquire site lock\n",
    "# ==================================================\n",
    "acquire_site_lock(\n",
    "    spark=spark,\n",
    "    site_id=SITE_ID,\n",
    "    locked_by=JOB_RUN_ID,\n",
    "    ttl_minutes=120\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ==================================================\n",
    "    # 1) Read water cells from surface_cells_v2\n",
    "    #    surface_cells_v2 must include:\n",
    "    #      siteId, tileId, cellX, cellY, waterPointRatio, cellSizeM, minZ, meanZ, maxZ\n",
    "    # ==================================================\n",
    "    df_cells = (\n",
    "        spark.table(CELLS_TABLE)\n",
    "             .filter(F.col(\"siteId\") == SITE_ID)\n",
    "             .filter(F.col(\"waterPointRatio\") >= F.lit(WATER_CELL_THRESHOLD))\n",
    "             .select(\n",
    "                 \"siteId\",\n",
    "                 \"tileId\",\n",
    "                 \"cellX\", \"cellY\",\n",
    "                 \"cellSizeM\",\n",
    "                 \"minZ\", \"meanZ\", \"maxZ\",\n",
    "                 \"waterPointRatio\"\n",
    "             )\n",
    "    )\n",
    "\n",
    "    if df_cells.rdd.isEmpty():\n",
    "        raise RuntimeError(f\"No water cells found for siteId={SITE_ID} with threshold={WATER_CELL_THRESHOLD}\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 2) Parse tileX/tileY from tileId = \"tileX_tileY\"\n",
    "    # ==================================================\n",
    "    df_cells = (\n",
    "        df_cells\n",
    "        .withColumn(\"tileX\", F.split(F.col(\"tileId\"), \"_\").getItem(0).cast(\"int\"))\n",
    "        .withColumn(\"tileY\", F.split(F.col(\"tileId\"), \"_\").getItem(1).cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 3) Compute global grid coordinates so cross-tile neighbors differ by 1\n",
    "    # ==================================================\n",
    "    df_cells_global = (\n",
    "        df_cells\n",
    "        .withColumn(\"globalCellX\", F.col(\"tileX\") * F.lit(cellsPerTile) + F.col(\"cellX\"))\n",
    "        .withColumn(\"globalCellY\", F.col(\"tileY\") * F.lit(cellsPerTile) + F.col(\"cellY\"))\n",
    "        .withColumn(\"vertexId\", F.concat_ws(\"_\", F.col(\"globalCellX\"), F.col(\"globalCellY\")))\n",
    "    )\n",
    "\n",
    "    # Vertices must have column \"id\" for GraphFrames\n",
    "    vertices = df_cells_global.select(F.col(\"vertexId\").alias(\"id\")).distinct()\n",
    "\n",
    "    # ==================================================\n",
    "    # 4) Build adjacency edges efficiently (no heavy self-join)\n",
    "    #    Approach:\n",
    "    #      - For each vertex, generate its neighbor coordinate candidates\n",
    "    #      - Join those candidates back to existing vertices by (globalCellX, globalCellY)\n",
    "    # ==================================================\n",
    "    v = df_cells_global.select(\"vertexId\", \"globalCellX\", \"globalCellY\").alias(\"v\")\n",
    "\n",
    "    neighbours = [\n",
    "        F.struct((F.col(\"globalCellX\") + 1).alias(\"x\"), F.col(\"globalCellY\").alias(\"y\")),\n",
    "        F.struct((F.col(\"globalCellX\") - 1).alias(\"x\"), F.col(\"globalCellY\").alias(\"y\")),\n",
    "        F.struct(F.col(\"globalCellX\").alias(\"x\"), (F.col(\"globalCellY\") + 1).alias(\"y\")),\n",
    "        F.struct(F.col(\"globalCellX\").alias(\"x\"), (F.col(\"globalCellY\") - 1).alias(\"y\")),\n",
    "    ]\n",
    "\n",
    "    if USE_EIGHT_NEIGHBOUR:\n",
    "        neighbours += [\n",
    "            F.struct((F.col(\"globalCellX\") + 1).alias(\"x\"), (F.col(\"globalCellY\") + 1).alias(\"y\")),\n",
    "            F.struct((F.col(\"globalCellX\") + 1).alias(\"x\"), (F.col(\"globalCellY\") - 1).alias(\"y\")),\n",
    "            F.struct((F.col(\"globalCellX\") - 1).alias(\"x\"), (F.col(\"globalCellY\") + 1).alias(\"y\")),\n",
    "            F.struct((F.col(\"globalCellX\") - 1).alias(\"x\"), (F.col(\"globalCellY\") - 1).alias(\"y\")),\n",
    "        ]\n",
    "\n",
    "    nbr = (\n",
    "        df_cells_global\n",
    "        .select(\n",
    "            F.col(\"vertexId\").alias(\"src\"),\n",
    "            F.explode(F.array(*neighbours)).alias(\"nbr\")\n",
    "        )\n",
    "        .select(\n",
    "            \"src\",\n",
    "            F.col(\"nbr.x\").alias(\"nx\"),\n",
    "            F.col(\"nbr.y\").alias(\"ny\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    edges = (\n",
    "        nbr.join(v, (F.col(\"nx\") == F.col(\"v.globalCellX\")) & (F.col(\"ny\") == F.col(\"v.globalCellY\")), \"inner\")\n",
    "           .select(F.col(\"src\"), F.col(\"v.vertexId\").alias(\"dst\"))\n",
    "           .distinct()\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 5) Connected components => waterBodyId\n",
    "    # ==================================================\n",
    "    g = GraphFrame(vertices, edges)\n",
    "    components = g.connectedComponents()  # returns: id, component\n",
    "\n",
    "    df_labeled = (\n",
    "        df_cells_global\n",
    "        .join(components, df_cells_global.vertexId == components.id, \"inner\")\n",
    "        .withColumnRenamed(\"component\", \"waterBodyId\")\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 6) Aggregate water body features\n",
    "    #    Note: bboxMin/Max are in global cell coordinates; you can convert to meters if needed.\n",
    "    # ==================================================\n",
    "    df_features = (\n",
    "        df_labeled\n",
    "        .groupBy(\"siteId\", \"waterBodyId\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"cellCount\"),\n",
    "            (F.count(\"*\") * F.first(\"cellSizeM\") * F.first(\"cellSizeM\")).alias(\"areaM2\"),\n",
    "            F.min(\"minZ\").alias(\"minZ\"),\n",
    "            F.max(\"maxZ\").alias(\"maxZ\"),\n",
    "            F.avg(\"meanZ\").alias(\"meanZ\"),\n",
    "            F.min(\"globalCellX\").alias(\"bboxMinCellX\"),\n",
    "            F.min(\"globalCellY\").alias(\"bboxMinCellY\"),\n",
    "            F.max(\"globalCellX\").alias(\"bboxMaxCellX\"),\n",
    "            F.max(\"globalCellY\").alias(\"bboxMaxCellY\")\n",
    "        )\n",
    "        .withColumn(\"computedAt\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    # Optional: drop tiny components (noise). Keep off by default.\n",
    "    # MIN_CELLS = int(spark.conf.get(\"pipeline.waterBodyMinCells\", \"0\"))\n",
    "    # if MIN_CELLS > 0:\n",
    "    #     df_features = df_features.filter(F.col(\"cellCount\") >= F.lit(MIN_CELLS))\n",
    "\n",
    "    # ==================================================\n",
    "    # 7) Write latest snapshot by site\n",
    "    # ==================================================\n",
    "    (\n",
    "        df_features.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"replaceWhere\", f\"siteId = '{SITE_ID}'\")\n",
    "            .option(\"path\", OUTPUT_PATH)\n",
    "            .partitionBy(\"siteId\")\n",
    "            .saveAsTable(OUTPUT_TABLE)\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 8) Quick verification\n",
    "    # ==================================================\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          COUNT(*) AS waterBodies,\n",
    "          SUM(cellCount) AS totalCells,\n",
    "          SUM(areaM2) AS totalAreaM2\n",
    "        FROM {OUTPUT_TABLE}\n",
    "        WHERE siteId = '{SITE_ID}'\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          waterBodyId, cellCount, areaM2\n",
    "        FROM {OUTPUT_TABLE}\n",
    "        WHERE siteId = '{SITE_ID}'\n",
    "        ORDER BY areaM2 DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    print(\"‚úÖ features_water_bodies_v2 complete.\")\n",
    "\n",
    "finally:\n",
    "    # Clean up checkpoint directory\n",
    "    try:\n",
    "        dbutils.fs.rm(CHECKPOINT_PATH, recurse=True)\n",
    "    except:\n",
    "        pass  # Ignore cleanup errors\n",
    "\n",
    "    release_site_lock(\n",
    "        spark=spark,\n",
    "        site_id=SITE_ID,\n",
    "        locked_by=JOB_RUN_ID\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_feature_water_bodies_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
