{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a7c7e9e-685e-462a-a29e-b5eff9fe6f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are a Databricks Workflows expert. Help me create a production-grade Job DAG in Databricks Workflows UI for my pipeline in catalog main, schema demo.\n",
    "\n",
    "Goal:\n",
    "Raw → tiling → tile stats → surface cells → surface patches → features → table optimization.\n",
    "\n",
    "Tasks (in order) and dependencies:\n",
    "1) 01_ingest_raw  (no dependency)\n",
    "2) 02_spatial_tiling_v2 depends on 01_ingest_raw\n",
    "3) 03_tile_stats_v2 depends on 02_spatial_tiling_v2\n",
    "4) 04_surface_cells_v2 depends on 03_tile_stats_v2\n",
    "5) 04_surface_patches_v2 depends on 04_surface_cells_v2\n",
    "6) 05_feature_water_bodies_v2 depends on 04_surface_cells_v2\n",
    "7) 05_feature_building_candidates_v2 depends on 04_surface_patches_v2\n",
    "8) 99_optimize_tables (SQL task) depends on both feature tasks\n",
    "\n",
    "Tables involved:\n",
    "points_raw\n",
    "processed_points_tiled_v2\n",
    "tile_stats_v2\n",
    "surface_cells_v2\n",
    "surface_patches_v2\n",
    "features_water_bodies_v2\n",
    "features_building_candidates_v2\n",
    "\n",
    "Job parameters (with defaults):\n",
    "siteId = \"wellington_cbd\"\n",
    "cellSizeM = \"0.5\"\n",
    "patchWaterThreshold = \"0.7\"\n",
    "skipWaterTileRatio = \"0.8\"\n",
    "targetIngestRunId = \"\" (optional)\n",
    "\n",
    "Requirements:\n",
    "- Use a job cluster\n",
    "- Set max concurrent runs = 1\n",
    "- Enable task retries (2-3) and a reasonable timeout per task\n",
    "- Pass parameters to scripts via spark.conf (e.g., pipeline.siteId) or notebook widgets\n",
    "- Put the SQL OPTIMIZE/ZORDER step at the end\n",
    "- Provide step-by-step UI instructions to configure each task and recommended compute settings\n",
    "\n",
    "Ask me for the workspace paths of each script if needed and show placeholders like <PATH_TO_SCRIPT>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53a84618-4cc4-4f71-a710-45e260f46372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Workflows Job DAG - Step-by-Step Configuration Guide\n",
    "\n",
    "## Overview\n",
    "This guide walks you through creating a production-grade Job DAG for the Trimble Geospatial point cloud processing pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Create the Job\n",
    "\n",
    "### Step 1.1: Navigate to Workflows\n",
    "1. Click **Workflows** in the left sidebar\n",
    "2. Click **Create Job** button (top right)\n",
    "3. Enter Job Name: `Geospatial_Pipeline_Wellington_CBD`\n",
    "\n",
    "### Step 1.2: Configure Job-Level Settings\n",
    "1. Click **Edit** next to the job name\n",
    "2. Set **Max concurrent runs**: `1` (prevents concurrent processing of same site)\n",
    "3. Click **Advanced** (if available)\n",
    "   - **Timeout**: `7200` seconds (2 hours for entire pipeline)\n",
    "4. Click **Save**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Configure Job Cluster\n",
    "\n",
    "### Step 2.1: Create Job Cluster\n",
    "1. In the job configuration page, scroll to **Compute** section\n",
    "2. Click **Add** or **Edit compute**\n",
    "3. Select **Job cluster** (not All-purpose cluster)\n",
    "\n",
    "### Step 2.2: Cluster Configuration\n",
    "**Cluster Name**: `geospatial-pipeline-cluster`\n",
    "\n",
    "**Cluster Mode**: `Standard`\n",
    "\n",
    "**Databricks Runtime**: `16.4 LTS (Scala 2.12, Spark 3.5.3)` with Photon\n",
    "\n",
    "**Node Type**:\n",
    "- **Driver**: `Standard_D4ads_v6` (4 cores, 16 GB RAM)\n",
    "- **Workers**: `Standard_D4ads_v6` (4 cores, 16 GB RAM)\n",
    "- **Workers count**: Start with `2-4` (autoscaling enabled)\n",
    "\n",
    "**Advanced Options**:\n",
    "- **Spark Config**:\n",
    "  ```\n",
    "  spark.databricks.delta.optimizeWrite.enabled true\n",
    "  spark.databricks.delta.autoCompact.enabled true\n",
    "  spark.sql.adaptive.enabled true\n",
    "  ```\n",
    "- **Environment Variables** (optional):\n",
    "  ```\n",
    "  PYSPARK_PYTHON=/databricks/python3/bin/python3\n",
    "  ```\n",
    "\n",
    "4. Click **Confirm**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Configure Job Parameters\n",
    "\n",
    "### Step 3.1: Add Job Parameters\n",
    "1. Scroll to **Parameters** section\n",
    "2. Click **Add parameter** for each:\n",
    "\n",
    "| Parameter Name | Default Value | Description |\n",
    "|----------------|---------------|-------------|\n",
    "| `siteId` | `wellington_cbd` | Site identifier |\n",
    "| `cellSizeM` | `0.5` | Surface cell size in meters |\n",
    "| `patchWaterThreshold` | `0.7` | Water threshold for patches |\n",
    "| `skipWaterTileRatio` | `0.8` | Skip tiles with >80% water |\n",
    "| `targetIngestRunId` | `` | Optional: specific ingest run |\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Add Tasks (8 Tasks Total)\n",
    "\n",
    "### Task 1: Ingest Raw Data\n",
    "\n",
    "**Click \"Add task\" button**\n",
    "\n",
    "**Task Configuration**:\n",
    "- **Task name**: `01_ingest_raw`\n",
    "- **Type**: `Notebook`\n",
    "- **Source**: `Workspace`\n",
    "- **Path**: `/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/01_ingest/01_ingest_raw`\n",
    "- **Cluster**: Select the job cluster created above\n",
    "- **Depends on**: (none - this is the first task)\n",
    "- **Timeout**: `1800` seconds (30 min)\n",
    "- **Retries**: `2`\n",
    "- **Retry interval**: `60` seconds\n",
    "\n",
    "**Parameters** (click \"Add\" under Base parameters):\n",
    "```\n",
    "siteId: {{job.parameters.siteId}}\n",
    "targetIngestRunId: {{job.parameters.targetIngestRunId}}\n",
    "```\n",
    "\n",
    "**Click \"Create task\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Task 2: Spatial Tiling\n",
    "\n",
    "**Click \"Add task\" button**\n",
    "\n",
    "**Task Configuration**:\n",
    "- **Task name**: `02_spatial_tiling_v2`\n",
    "- **Type**: `Notebook`\n",
    "- **Source**: `Workspace`\n",
    "- **Path**: `/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/02_processing/02_spatial_tiling_v2`\n",
    "- **Cluster**: (same job cluster)\n",
    "- **Depends on**: `01_ingest_raw` ✅\n",
    "- **Timeout**: `3600` seconds (1 hour)\n",
    "- **Retries**: `2`\n",
    "- **Retry interval**: `60` seconds\n",
    "\n",
    "**Parameters**:\n",
    "```\n",
    "siteId: {{job.parameters.siteId}}\n",
    "```\n",
    "\n",
    "**Click \"Create task\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Task 3: Tile Statistics\n",
    "\n",
    "**Click \"Add task\" button**\n",
    "\n",
    "**Task Configuration**:\n",
    "- **Task name**: `03_tile_stats_v2`\n",
    "- **Type**: `Notebook`\n",
    "- **Source**: `Workspace`\n",
    "- **Path**: `/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/03_aggregation/03_tile_stats_v2`\n",
    "- **Cluster**: (same job cluster)\n",
    "- **Depends on**: `02_spatial_tiling_v2` ✅\n",
    "- **Timeout**: `1800` seconds (30 min)\n",
    "- **Retries**: `2`\n",
    "- **Retry interval**: `60` seconds\n",
    "\n",
    "**Parameters**:\n",
    "```\n",
    "siteId: {{job.parameters.siteId}}\n",
    "skipWaterTileRatio: {{job.parameters.skipWaterTileRatio}}\n",
    "```\n",
    "\n",
    "**Click \"Create task\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Task 4: Surface Cells\n",
    "\n",
    "**Click \"Add task\" button**\n",
    "\n",
    "**Task Configuration**:\n",
    "- **Task name**: `04_surface_cells_v2`\n",
    "- **Type**: `Notebook`\n",
    "- **Source**: `Workspace`\n",
    "- **Path**: `/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/04_surface/04_surface_cells_v2`\n",
    "- **Cluster**: (same job cluster)\n",
    "- **Depends on**: `03_tile_stats_v2` ✅\n",
    "- **Timeout**: `3600` seconds (1 hour)\n",
    "- **Retries**: `2`\n",
    "- **Retry interval**: `60` seconds\n",
    "\n",
    "**Parameters**:\n",
    "```\n",
    "siteId: {{job.parameters.siteId}}\n",
    "cellSizeM: {{job.parameters.cellSizeM}}\n",
    "```\n",
    "\n",
    "**Click \"Create task\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Task 5: Surface Patches\n",
    "\n",
    "**Click \"Add task\" button**\n",
    "\n",
    "**Task Configuration**:\n",
    "- **Task name**: `04_surface_patches_v2`\n",
    "- **Type**: `Notebook`\n",
    "- **Source**: `Workspace`\n",
    "- **Path**: `/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/04_surface/04_surface_patches_v2`\n",
    "- **Cluster**: (same job cluster)\n",
    "- **Depends on**: `04_surface_cells_v2` ✅\n",
    "- **Timeout**: `1800` seconds (30 min)\n",
    "- **Retries**: `2`\n",
    "- **Retry interval**: `60` seconds\n",
    "\n",
    "**Parameters**:\n",
    "```\n",
    "siteId: {{job.parameters.siteId}}\n",
    "patchWaterThreshold: {{job.parameters.patchWaterThreshold}}\n",
    "```\n",
    "\n",
    "**Click \"Create task\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Task 6: Extract Water Bodies\n",
    "\n",
    "**Click \"Add task\" button**\n",
    "\n",
    "**Task Configuration**:\n",
    "- **Task name**: `05_feature_water_bodies_v2`\n",
    "- **Type**: `Notebook`\n",
    "- **Source**: `Workspace`\n",
    "- **Path**: `/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/05_feature/05_feature_water_bodies_v2`\n",
    "- **Cluster**: (same job cluster)\n",
    "- **Depends on**: `04_surface_cells_v2` ✅ (parallel with patches)\n",
    "- **Timeout**: `1800` seconds (30 min)\n",
    "- **Retries**: `2`\n",
    "- **Retry interval**: `60` seconds\n",
    "\n",
    "**Parameters**:\n",
    "```\n",
    "siteId: {{job.parameters.siteId}}\n",
    "```\n",
    "\n",
    "**Click \"Create task\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Task 7: Extract Building Candidates\n",
    "\n",
    "**Click \"Add task\" button**\n",
    "\n",
    "**Task Configuration**:\n",
    "- **Task name**: `05_feature_building_candidates_v2`\n",
    "- **Type**: `Notebook`\n",
    "- **Source**: `Workspace`\n",
    "- **Path**: `/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/05_feature/05_feature_building_candidates_v2`\n",
    "- **Cluster**: (same job cluster)\n",
    "- **Depends on**: `04_surface_patches_v2` ✅\n",
    "- **Timeout**: `1800` seconds (30 min)\n",
    "- **Retries**: `2`\n",
    "- **Retry interval**: `60` seconds\n",
    "\n",
    "**Parameters**:\n",
    "```\n",
    "siteId: {{job.parameters.siteId}}\n",
    "```\n",
    "\n",
    "**Click \"Create task\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Task 8: Optimize Tables (SQL)\n",
    "\n",
    "**Click \"Add task\" button**\n",
    "\n",
    "**Task Configuration**:\n",
    "- **Task name**: `99_optimize_tables`\n",
    "- **Type**: `Notebook` (or SQL if you have a .sql file)\n",
    "- **Source**: `Workspace`\n",
    "- **Path**: `/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/99_optimization/99_optimize_tables`\n",
    "- **Cluster**: (same job cluster)\n",
    "- **Depends on**: \n",
    "  - `05_feature_water_bodies_v2` ✅\n",
    "  - `05_feature_building_candidates_v2` ✅\n",
    "  - (Click \"Add dependency\" to add both)\n",
    "- **Timeout**: `1800` seconds (30 min)\n",
    "- **Retries**: `1`\n",
    "- **Retry interval**: `60` seconds\n",
    "\n",
    "**Parameters**:\n",
    "```\n",
    "siteId: {{job.parameters.siteId}}\n",
    "```\n",
    "\n",
    "**Click \"Create task\"**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Verify DAG Structure\n",
    "\n",
    "### Expected DAG Flow:\n",
    "```\n",
    "01_ingest_raw\n",
    "      ↓\n",
    "02_spatial_tiling_v2\n",
    "      ↓\n",
    "03_tile_stats_v2\n",
    "      ↓\n",
    "04_surface_cells_v2\n",
    "      ↓               ↘\n",
    "04_surface_patches_v2   05_feature_water_bodies_v2\n",
    "      ↓                           ↓\n",
    "05_feature_building_candidates_v2 ↓\n",
    "      ↓                           ↓\n",
    "      └─────→ 99_optimize_tables ←┘\n",
    "```\n",
    "\n",
    "### Verification Steps:\n",
    "1. Click **Graph** tab to view the DAG visualization\n",
    "2. Verify all dependencies are correct\n",
    "3. Check that Task 6 and Task 7 can run in parallel\n",
    "4. Confirm Task 8 waits for both feature tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Configure Notifications (Optional)\n",
    "\n",
    "### Step 6.1: Add Email Alerts\n",
    "1. Scroll to **Notifications** section\n",
    "2. Click **Add notification**\n",
    "3. Configure:\n",
    "   - **On failure**: Send email to `jinapang2003@gmail.com`\n",
    "   - **On success**: (optional) Send email\n",
    "   - **On start**: (optional)\n",
    "\n",
    "---\n",
    "\n",
    "## Part 7: Save and Test\n",
    "\n",
    "### Step 7.1: Save the Job\n",
    "1. Click **Save** (top right)\n",
    "2. Review the job configuration summary\n",
    "\n",
    "### Step 7.2: Test Run\n",
    "1. Click **Run now** button\n",
    "2. Monitor the run in the **Runs** tab\n",
    "3. Check each task's logs for errors\n",
    "4. Verify data in Unity Catalog tables:\n",
    "   ```sql\n",
    "   SELECT COUNT(*) FROM main.demo.points_raw WHERE siteId = 'wellington_cbd';\n",
    "   SELECT COUNT(*) FROM main.demo.processed_points_tiled_v2 WHERE siteId = 'wellington_cbd';\n",
    "   SELECT COUNT(*) FROM main.demo.tile_stats_v2 WHERE siteId = 'wellington_cbd';\n",
    "   ```\n",
    "\n",
    "### Step 7.3: Schedule (Optional)\n",
    "1. Click **Add trigger** in the job page\n",
    "2. Choose:\n",
    "   - **Scheduled**: Cron expression (e.g., `0 2 * * *` for 2 AM daily)\n",
    "   - **File arrival**: Trigger on new files in landing zone\n",
    "   - **Continuous**: For streaming pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Part 8: Production Checklist\n",
    "\n",
    "### Before Production Deployment:\n",
    "- [ ] All notebooks have proper error handling\n",
    "- [ ] Site lock mechanism is tested\n",
    "- [ ] External locations are configured\n",
    "- [ ] Job parameters are documented\n",
    "- [ ] Cluster size is right-sized for data volume\n",
    "- [ ] Retry logic is appropriate\n",
    "- [ ] Notifications are configured\n",
    "- [ ] Access controls are set (who can run/modify the job)\n",
    "- [ ] Cost monitoring is enabled\n",
    "- [ ] Backup/rollback plan is documented\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting Tips\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**Issue 1: Task fails with \"No parent external location\"**\n",
    "- **Solution**: Run setup notebooks in `00_setup/` first\n",
    "- Verify external locations exist: `SHOW EXTERNAL LOCATIONS;`\n",
    "\n",
    "**Issue 2: Site lock timeout**\n",
    "- **Solution**: Check if another job is holding the lock\n",
    "- Manually release: Run `release_site_lock()` in a notebook\n",
    "\n",
    "**Issue 3: Out of memory errors**\n",
    "- **Solution**: Increase worker node size or count\n",
    "- Enable adaptive query execution (already in Spark config)\n",
    "\n",
    "**Issue 4: Slow performance**\n",
    "- **Solution**: Run `99_optimize_tables` more frequently\n",
    "- Check partition pruning in queries\n",
    "- Review Spark UI for skewed partitions\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Follow this guide** to create the job in Databricks UI\n",
    "2. **Test with small dataset** first (single site)\n",
    "3. **Monitor first production run** closely\n",
    "4. **Optimize cluster size** based on actual resource usage\n",
    "5. **Set up alerting** for failures\n",
    "6. **Document runbook** for on-call engineers\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference: Notebook Paths\n",
    "\n",
    "```\n",
    "/Users/jinapang2003@gmail.com/trimble-geospacial-demo/databricks/pipelines/\n",
    "├── 01_ingest/01_ingest_raw.ipynb\n",
    "├── 02_processing/02_spatial_tiling_v2.ipynb\n",
    "├── 03_aggregation/03_tile_stats_v2.ipynb\n",
    "├── 04_surface/04_surface_cells_v2.ipynb\n",
    "├── 04_surface/04_surface_patches_v2.ipynb\n",
    "├── 05_feature/05_feature_water_bodies_v2.ipynb\n",
    "├── 05_feature/05_feature_building_candidates_v2.ipynb\n",
    "└── 99_optimization/99_optimize_tables.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b12093-292a-4d9a-b702-510ddb66b3ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Trimble Geospatial Demo - Pipelines Directory Structure\n",
    "\n",
    "## Overview\n",
    "This directory contains the complete point cloud processing pipeline for geospatial data analysis.\n",
    "\n",
    "## Directory Tree\n",
    "\n",
    "```\n",
    "pipelines/\n",
    "│\n",
    "├── 00_setup/                                    # Initial setup and configuration\n",
    "│   ├── create_site_lock_table.ipynb            # Create distributed locking table\n",
    "│   ├── setup_external_locations.ipynb          # Register Unity Catalog external locations\n",
    "│   └── setup_tiling_params.ipynb               # Configure spatial tiling parameters\n",
    "│\n",
    "├── 01_ingest/                                   # Data ingestion layer\n",
    "│   ├── 01_ingest_raw.ipynb                     # Ingest raw point cloud data from landing zone\n",
    "│   └── lesson learned.ipynb                    # Documentation of ingestion insights\n",
    "│\n",
    "├── 02_processing/                               # Data processing and transformation\n",
    "│   ├── 02_spatial_tiling_v2.ipynb              # Spatial tiling with H3/custom grid\n",
    "│   └── benchmark_to_control_params.ipynb       # Performance tuning and parameter optimization\n",
    "│\n",
    "├── 03_aggregation/                              # Statistical aggregation\n",
    "│   └── 03_tile_stats_v2.ipynb                  # Compute per-tile statistics (z, intensity, water)\n",
    "│\n",
    "├── 04_surface/                                  # Surface reconstruction\n",
    "│   ├── 04_surface_cells_v2.ipynb               # Generate surface grid cells\n",
    "│   └── 04_surface_patches_v2.ipynb             # Create surface patches from cells\n",
    "│\n",
    "├── 05_feature/                                  # Feature extraction\n",
    "│   ├── 05_feature_building_candidates_v2.ipynb # Detect building candidates\n",
    "│   ├── 05_feature_water_bodies_v2.ipynb        # Extract water body features\n",
    "│   └── debug.ipynb                             # Feature extraction debugging\n",
    "│\n",
    "├── 99_optimization/                             # Table maintenance and optimization\n",
    "│   ├── 99_optimize_tables.ipynb                # Run OPTIMIZE on Delta tables\n",
    "│   ├── analyze_tables.ipynb                    # Compute table statistics\n",
    "│   └── vacuum_tables.ipynb                     # Clean up old Delta versions\n",
    "│\n",
    "└── path_structure.ipynb                         # Documentation of storage paths\n",
    "```\n",
    "\n",
    "## Pipeline Stages\n",
    "\n",
    "### Stage 0: Setup (One-Time)\n",
    "* **Purpose**: Initialize infrastructure and configuration\n",
    "* **Notebooks**: 3\n",
    "* **Run Frequency**: Once per workspace/site\n",
    "\n",
    "### Stage 1: Ingestion\n",
    "* **Purpose**: Load raw point cloud data into Delta Lake\n",
    "* **Input**: Parquet files from landing zone\n",
    "* **Output**: `main.demo.points_raw` (partitioned by siteId, ingestRunId)\n",
    "\n",
    "### Stage 2: Processing\n",
    "* **Purpose**: Spatial indexing and tiling\n",
    "* **Input**: `points_raw`\n",
    "* **Output**: `processed_points_tiled_v2` (partitioned by siteId, tileId)\n",
    "\n",
    "### Stage 3: Aggregation\n",
    "* **Purpose**: Compute tile-level statistics\n",
    "* **Input**: `processed_points_tiled_v2`\n",
    "* **Output**: `tile_stats_v2` (includes water detection)\n",
    "\n",
    "### Stage 4: Surface Reconstruction\n",
    "* **Purpose**: Generate digital surface models\n",
    "* **Input**: `processed_points_tiled_v2`, `tile_stats_v2`\n",
    "* **Output**: `surface_cells_v2`, `surface_patches_v2`\n",
    "\n",
    "### Stage 5: Feature Extraction\n",
    "* **Purpose**: Identify buildings, water bodies, and other features\n",
    "* **Input**: `surface_cells_v2`, `surface_patches_v2`\n",
    "* **Output**: `building_candidates_v2`, `water_bodies_v2`\n",
    "\n",
    "### Stage 99: Optimization\n",
    "* **Purpose**: Maintain Delta table performance\n",
    "* **Run Frequency**: Weekly or after major ingestion\n",
    "\n",
    "## Key Design Patterns\n",
    "\n",
    "### 1. Site-Level Locking\n",
    "* All processing notebooks use `acquire_site_lock()` / `release_site_lock()`\n",
    "* Prevents concurrent writes to the same site\n",
    "* Ensures latest-snapshot semantics\n",
    "\n",
    "### 2. External Table Pattern\n",
    "* All tables use Unity Catalog external locations\n",
    "* Storage paths: `abfss://raw@...`, `abfss://processed@...`, `abfss://aggregated@...`\n",
    "* Enables cross-workspace data sharing\n",
    "\n",
    "### 3. Versioned Notebooks\n",
    "* `_v2` suffix indicates second iteration\n",
    "* Allows A/B testing and gradual migration\n",
    "\n",
    "### 4. Partitioning Strategy\n",
    "* **Raw/Processed**: `siteId` + `ingestRunId` or `tileId`\n",
    "* **Aggregated**: `siteId` + derived keys\n",
    "* Enables efficient `replaceWhere` operations\n",
    "\n",
    "## Storage Locations\n",
    "\n",
    "| Layer | Container | External Location | Example Path |\n",
    "|-------|-----------|-------------------|-------------|\n",
    "| Raw | `raw` | `raw_container` | `abfss://raw@trimblegeospatialdemo.../points` |\n",
    "| Processed | `processed` | `processed_container` | `abfss://processed@.../points_tiled_v2` |\n",
    "| Aggregated | `aggregated` | `aggregated_container` | `abfss://aggregated@.../tile_stats_v2` |\n",
    "\n",
    "## Execution Order\n",
    "\n",
    "```\n",
    "00_setup (once)\n",
    "    ↓\n",
    "01_ingest_raw\n",
    "    ↓\n",
    "02_spatial_tiling_v2\n",
    "    ↓\n",
    "03_tile_stats_v2\n",
    "    ↓\n",
    "04_surface_cells_v2 → 04_surface_patches_v2\n",
    "    ↓\n",
    "05_feature_building_candidates_v2\n",
    "05_feature_water_bodies_v2\n",
    "    ↓\n",
    "99_optimize_tables (periodic)\n",
    "```\n",
    "\n",
    "## Notes\n",
    "\n",
    "* All notebooks use `main.demo` catalog/schema\n",
    "* Water classification code: `9` (LAS standard)\n",
    "* Mostly water threshold: `0.80` (80% water points)\n",
    "* Site lock TTL: 60 minutes (configurable)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dag_instruction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
