{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff46ea7-3331-4792-8e93-84ce9a90e0b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 04_surface_cells_v2.py\n",
    "from pyspark.sql import functions as F\n",
    "from utils.site_lock import acquire_site_lock, release_site_lock\n",
    "\n",
    "# ==================================================\n",
    "# Unity Catalog context\n",
    "# ==================================================\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA demo\")\n",
    "\n",
    "# ==================================================\n",
    "# CONFIG\n",
    "# ==================================================\n",
    "SITE_ID = spark.conf.get(\"pipeline.siteId\", \"wellington_cbd\")\n",
    "\n",
    "POINTS_TABLE   = \"processed_points_tiled_v2\"\n",
    "TILE_STATS_TBL = \"tile_stats_v2\"\n",
    "\n",
    "OUTPUT_TABLE = \"surface_cells_v2\"\n",
    "OUTPUT_PATH  = \"abfss://processed@trimblegeospatialdemo.dfs.core.windows.net/surface_cells_v2\"\n",
    "\n",
    "# Cell resolution (meters). Choose 0.25 / 0.5 / 1.0 depending on density + cost.\n",
    "CELL_SIZE_M = 0.50\n",
    "\n",
    "# Skip tiles dominated by water (keep shoreline/mixed tiles)\n",
    "SKIP_WATER_RATIO_GE = float(spark.conf.get(\"pipeline.skipWaterTileRatio\", \"0.8\"))\n",
    "\n",
    "# Optional: skip tiny tiles (saves cost + avoids noisy cells)\n",
    "MIN_TILE_POINTS = 5_000\n",
    "\n",
    "# ==================================================\n",
    "# Job identity (for locking / audit)\n",
    "# ==================================================\n",
    "JOB_RUN_ID = spark.conf.get(\"spark.databricks.job.runId\", \"manual-notebook\")\n",
    "\n",
    "# ==================================================\n",
    "# Acquire site-level lock (latest snapshot semantics)\n",
    "# ==================================================\n",
    "acquire_site_lock(\n",
    "    spark=spark,\n",
    "    site_id=SITE_ID,\n",
    "    locked_by=JOB_RUN_ID,\n",
    "    ttl_minutes=90\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ==================================================\n",
    "    # 1) Select tiles to process (routing)\n",
    "    #    - skip mostly-water tiles (waterRatio >= 0.7)\n",
    "    #    - optionally skip tiles with very few points\n",
    "    # ==================================================\n",
    "    df_tiles = (\n",
    "        spark.table(TILE_STATS_TBL)\n",
    "             .filter(F.col(\"siteId\") == SITE_ID)\n",
    "             .filter(F.col(\"waterPointRatio\") < F.lit(SKIP_WATER_RATIO_GE))\n",
    "             .filter(F.col(\"pointCount\") >= F.lit(MIN_TILE_POINTS))\n",
    "             .select(\"siteId\", \"tileId\")\n",
    "             .distinct()\n",
    "    )\n",
    "\n",
    "    if df_tiles.rdd.isEmpty():\n",
    "        raise RuntimeError(\n",
    "            f\"No tiles eligible for processing for siteId={SITE_ID}. \"\n",
    "            f\"Check water ratio threshold / MIN_TILE_POINTS.\"\n",
    "        )\n",
    "\n",
    "    # Broadcast tile list (typically small)\n",
    "    df_tiles_b = F.broadcast(df_tiles)\n",
    "\n",
    "    # ==================================================\n",
    "    # 2) Read points for selected tiles only\n",
    "    # ==================================================\n",
    "    df_points = (\n",
    "        spark.table(POINTS_TABLE)\n",
    "             .filter(F.col(\"siteId\") == SITE_ID)\n",
    "             .join(df_tiles_b, [\"siteId\", \"tileId\"], \"inner\")\n",
    "             .select(\n",
    "                 \"siteId\", \"tileId\",\n",
    "                 \"x\", \"y\", \"z\",\n",
    "                 \"originX\", \"originY\", \"tileSizeM\",\n",
    "                 \"tileX\", \"tileY\",\n",
    "                 \"classification\", \"intensity\"\n",
    "             )\n",
    "    )\n",
    "\n",
    "    if df_points.rdd.isEmpty():\n",
    "        raise RuntimeError(f\"No point data found after routing join for siteId={SITE_ID}\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 3) Compute per-tile origin for cell indexing\n",
    "    #\n",
    "    # We want cell coords *within* each tile:\n",
    "    #   localX = x - (originX + tileX * tileSizeM)\n",
    "    #   localY = y - (originY + tileY * tileSizeM)\n",
    "    #\n",
    "    # Then:\n",
    "    #   cellX = floor(localX / CELL_SIZE_M)\n",
    "    #   cellY = floor(localY / CELL_SIZE_M)\n",
    "    #\n",
    "    # This avoids floating drift and makes cells stable inside a tile.\n",
    "    # ==================================================\n",
    "    df_cells_keyed = (\n",
    "        df_points\n",
    "        .withColumn(\"tileOriginX\", F.col(\"originX\") + (F.col(\"tileX\") * F.col(\"tileSizeM\")))\n",
    "        .withColumn(\"tileOriginY\", F.col(\"originY\") + (F.col(\"tileY\") * F.col(\"tileSizeM\")))\n",
    "        .withColumn(\"localX\", F.col(\"x\") - F.col(\"tileOriginX\"))\n",
    "        .withColumn(\"localY\", F.col(\"y\") - F.col(\"tileOriginY\"))\n",
    "        .withColumn(\"cellSizeM\", F.lit(float(CELL_SIZE_M)))\n",
    "        .withColumn(\"cellX\", F.floor(F.col(\"localX\") / F.lit(CELL_SIZE_M)).cast(\"int\"))\n",
    "        .withColumn(\"cellY\", F.floor(F.col(\"localY\") / F.lit(CELL_SIZE_M)).cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 4) Aggregate into cells\n",
    "    # ==================================================\n",
    "    df_surface_cells = (\n",
    "        df_cells_keyed\n",
    "        .groupBy(\"siteId\", \"tileId\", \"cellX\", \"cellY\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"pointCount\"),\n",
    "\n",
    "            # cell-level water count\n",
    "            F.sum(\n",
    "                F.when(F.col(\"classification\") == F.lit(WATER_CLASS), 1)\n",
    "                .otherwise(0)\n",
    "            ).alias(\"waterPointCount\"),\n",
    "\n",
    "            F.min(\"z\").alias(\"minZ\"),\n",
    "            F.avg(\"z\").alias(\"meanZ\"),\n",
    "            F.max(\"z\").alias(\"maxZ\"),\n",
    "            F.expr(\"percentile_approx(z, 0.50)\").alias(\"z_p50\")\n",
    "        )\n",
    "        # cell-level water ratio\n",
    "        .withColumn(\n",
    "            \"waterPointRatio\",\n",
    "            F.when(\n",
    "                F.col(\"pointCount\") > 0,\n",
    "                F.col(\"waterPointCount\") / F.col(\"pointCount\")\n",
    "            ).otherwise(F.lit(0.0))\n",
    "        )\n",
    "        .withColumn(\"cellSizeM\", F.lit(float(CELL_SIZE_M)))\n",
    "        .withColumn(\"computedAt\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 5) Safety check: ensure only one siteId\n",
    "    # ==================================================\n",
    "    if df_surface_cells.select(\"siteId\").distinct().count() != 1:\n",
    "        raise RuntimeError(\"surface_cells output contains multiple siteId values\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 6) Write latest snapshot (replace entire site)\n",
    "    # ==================================================\n",
    "    (\n",
    "        df_surface_cells.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"replaceWhere\", f\"siteId = '{SITE_ID}'\")\n",
    "            .option(\"path\", OUTPUT_PATH)\n",
    "            .partitionBy(\"siteId\", \"tileId\")\n",
    "            .saveAsTable(OUTPUT_TABLE)\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 7) Verification (high-level)\n",
    "    # ==================================================\n",
    "    print(\"\\n=== Verify surface_cells_v2 ===\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          siteId,\n",
    "          COUNT(*) AS cellRows,\n",
    "          SUM(pointCount) AS totalPointsUsed,\n",
    "          MIN(minZ) AS siteMinZ,\n",
    "          MAX(maxZ) AS siteMaxZ,\n",
    "          MIN(computedAt) AS minComputedAt,\n",
    "          MAX(computedAt) AS maxComputedAt\n",
    "        FROM {OUTPUT_TABLE}\n",
    "        WHERE siteId = '{SITE_ID}'\n",
    "        GROUP BY siteId\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    # Optional: how many tiles produced cells?\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          siteId,\n",
    "          COUNT(DISTINCT tileId) AS tilesWithCells\n",
    "        FROM {OUTPUT_TABLE}\n",
    "        WHERE siteId = '{SITE_ID}'\n",
    "        GROUP BY siteId\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    print(\"âœ… surface_cells_v2 written successfully\")\n",
    "\n",
    "finally:\n",
    "    # ==================================================\n",
    "    # Release site-level lock\n",
    "    # ==================================================\n",
    "    release_site_lock(\n",
    "        spark=spark,\n",
    "        site_id=SITE_ID,\n",
    "        locked_by=JOB_RUN_ID\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_surface_cells_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
