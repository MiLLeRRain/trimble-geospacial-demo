{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2994817-d104-4ac5-93ad-08595f6a9ba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 04_surface_patches_v2.py\n",
    "from pyspark.sql import functions as F\n",
    "from utils.site_lock import acquire_site_lock, release_site_lock\n",
    "\n",
    "# ==================================================\n",
    "# Unity Catalog context\n",
    "# ==================================================\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA demo\")\n",
    "\n",
    "# ==================================================\n",
    "# CONFIG\n",
    "# ==================================================\n",
    "SITE_ID = spark.conf.get(\"pipeline.siteId\", \"wellington_cbd\")\n",
    "\n",
    "CELLS_TABLE     = \"surface_cells_v2\"\n",
    "TILE_STATS_TBL  = \"tile_stats_v2\"\n",
    "\n",
    "OUTPUT_TABLE = \"surface_patches_v2\"\n",
    "OUTPUT_PATH  = \"abfss://processed@trimblegeospatialdemo.dfs.core.windows.net/surface_patches_v2\"\n",
    "\n",
    "# Patch granularity (coarse grouping inside a tile)\n",
    "# This defines the \"approximate patch\" size in number of cells\n",
    "PATCH_CELL_SIZE = 8   # e.g. 8x8 cells form one patch bucket\n",
    "\n",
    "# Water routing threshold (keep shoreline, skip pure water earlier)\n",
    "PATCH_WATER_THRESHOLD = float(spark.conf.get(\"pipeline.patchWaterThreshold\", \"0.7\"))\n",
    "\n",
    "# ==================================================\n",
    "# Job identity (for locking)\n",
    "# ==================================================\n",
    "JOB_RUN_ID = spark.conf.get(\n",
    "    \"spark.databricks.job.runId\",\n",
    "    \"manual-notebook\"\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# Acquire site-level lock\n",
    "# ==================================================\n",
    "acquire_site_lock(\n",
    "    spark=spark,\n",
    "    site_id=SITE_ID,\n",
    "    locked_by=JOB_RUN_ID,\n",
    "    ttl_minutes=90\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ==================================================\n",
    "    # 1) Read tile routing info (surface type signal)\n",
    "    # ==================================================\n",
    "    df_tile_stats = (\n",
    "        spark.table(TILE_STATS_TBL)\n",
    "             .filter(F.col(\"siteId\") == SITE_ID)\n",
    "             .select(\n",
    "                 \"siteId\",\n",
    "                 \"tileId\",\n",
    "                 \"waterPointRatio\"\n",
    "             )\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 2) Read surface cells and attach surfaceType\n",
    "    # ==================================================\n",
    "    df_cells = (\n",
    "        spark.table(CELLS_TABLE)\n",
    "             .filter(F.col(\"siteId\") == SITE_ID)\n",
    "             .join(df_tile_stats, [\"siteId\", \"tileId\"], \"inner\")\n",
    "             .select(\n",
    "                     \"siteId\", \"tileId\",\n",
    "                     \"cellX\", \"cellY\",\n",
    "                     \"pointCount\",\n",
    "                     \"waterPointCount\",\n",
    "                     \"waterPointRatio\",\n",
    "                     \"minZ\", \"meanZ\", \"maxZ\"\n",
    "                 )\n",
    "    )\n",
    "\n",
    "    if df_cells.rdd.isEmpty():\n",
    "        raise RuntimeError(f\"No surface cells found for siteId={SITE_ID}\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 3) Assign patch buckets (demo version)\n",
    "    #\n",
    "    # Patch = coarse bucket of cells inside a tile:\n",
    "    #   patchX = floor(cellX / PATCH_CELL_SIZE)\n",
    "    #   patchY = floor(cellY / PATCH_CELL_SIZE)\n",
    "    #\n",
    "    # This gives:\n",
    "    #   - stable patch IDs\n",
    "    #   - spatial locality\n",
    "    #   - predictable patch counts\n",
    "    #\n",
    "    # Later you can replace this with true connected-components.\n",
    "    # ==================================================\n",
    "    df_with_patches = (\n",
    "        df_cells\n",
    "        .withColumn(\"patchX\", F.floor(F.col(\"cellX\") / F.lit(PATCH_CELL_SIZE)).cast(\"int\"))\n",
    "        .withColumn(\"patchY\", F.floor(F.col(\"cellY\") / F.lit(PATCH_CELL_SIZE)).cast(\"int\"))\n",
    "        .withColumn(\n",
    "            \"patchId\",\n",
    "            F.concat_ws(\"_\", F.col(\"patchX\").cast(\"string\"), F.col(\"patchY\").cast(\"string\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 4) Aggregate to patch level\n",
    "    # ==================================================\n",
    "    df_surface_patches = (\n",
    "        df_with_patches\n",
    "        .groupBy(\"siteId\", \"tileId\", \"patchId\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"cellCount\"),\n",
    "            F.sum(\"pointCount\").alias(\"pointsUsed\"),\n",
    "\n",
    "            # patch-level water stats\n",
    "            F.sum(\"waterPointCount\").alias(\"waterPointCount\"),\n",
    "\n",
    "            F.min(\"minZ\").alias(\"minZ\"),\n",
    "            F.max(\"maxZ\").alias(\"maxZ\"),\n",
    "            F.avg(\"meanZ\").alias(\"meanZ\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"waterPointRatio\",\n",
    "            F.when(\n",
    "                F.col(\"pointsUsed\") > 0,\n",
    "                F.col(\"waterPointCount\") / F.col(\"pointsUsed\")\n",
    "            ).otherwise(F.lit(0.0))\n",
    "        )\n",
    "        # surfaceType from patches it self\n",
    "        .withColumn(\n",
    "            \"surfaceType\",\n",
    "            F.when(\n",
    "                F.col(\"waterPointRatio\") >= F.lit(PATCH_WATER_THRESHOLD),\n",
    "                F.lit(\"water\")\n",
    "            ).otherwise(F.lit(\"ground\"))\n",
    "        )\n",
    "        .withColumn(\"computedAt\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 5) Safety check\n",
    "    # ==================================================\n",
    "    if df_surface_patches.select(\"siteId\").distinct().count() != 1:\n",
    "        raise RuntimeError(\"surface_patches output contains multiple siteId values\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 6) Write latest snapshot (replace entire site)\n",
    "    # ==================================================\n",
    "    (\n",
    "        df_surface_patches.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"replaceWhere\", f\"siteId = '{SITE_ID}'\")\n",
    "            .option(\"path\", OUTPUT_PATH)\n",
    "            .partitionBy(\"siteId\", \"tileId\")\n",
    "            .saveAsTable(OUTPUT_TABLE)\n",
    "    )\n",
    "\n",
    "    # ==================================================\n",
    "    # 7) Verification / demo-friendly summary\n",
    "    # ==================================================\n",
    "    print(\"\\n=== Verify surface_patches_v2 ===\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "          surfaceType,\n",
    "          COUNT(DISTINCT patchId) AS patchCount,\n",
    "          SUM(pointsUsed)         AS pointsUsed\n",
    "        FROM {OUTPUT_TABLE}\n",
    "        WHERE siteId = '{SITE_ID}'\n",
    "        GROUP BY surfaceType\n",
    "        ORDER BY surfaceType\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "    print(\"âœ… surface_patches_v2 written successfully\")\n",
    "\n",
    "finally:\n",
    "    # ==================================================\n",
    "    # Release site-level lock\n",
    "    # ==================================================\n",
    "    release_site_lock(\n",
    "        spark=spark,\n",
    "        site_id=SITE_ID,\n",
    "        locked_by=JOB_RUN_ID\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_surface_patches_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
